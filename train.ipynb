{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962652cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 1. Setup & Training Function\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset, WeightedRandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import EfficientNet_V2_S_Weights\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "optimizations = []\n",
    "\n",
    "if DEVICE.type == 'cuda':\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        AMP_DTYPE = torch.bfloat16\n",
    "        optimizations.append(\"BFloat16\")\n",
    "    else:\n",
    "        raise RuntimeError(\"âŒ GPU tidak support BF16!\")\n",
    "        \n",
    "    if torch.backends.cuda.matmul.allow_tf32:\n",
    "        optimizations.append(\"TF32\")\n",
    "    if torch.backends.cudnn.benchmark:\n",
    "        optimizations.append(\"cuDNN Benchmark\")\n",
    "    optimizations.append(\"Channels Last\")\n",
    "\n",
    "print(f\"=== SYSTEM INFO ===\")\n",
    "print(f\"Device     : {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Optimasi   : {', '.join(optimizations)}\")\n",
    "print(\"===================\")\n",
    "\n",
    "class CheckpointWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self.module, x, use_reentrant=False)\n",
    "\n",
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform: x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, device, num_epochs, dataset_sizes, phase_name=\"Training\"): \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "    \n",
    "    print(f\"\\n--- Memulai {phase_name} ({num_epochs} Epochs) ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} [LR: {current_lr:.2e}]\")\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            \n",
    "            pbar = tqdm(dataloaders[phase], desc=f\"  {phase.upper()}\", leave=False)\n",
    "\n",
    "            for inputs, labels in pbar:\n",
    "                inputs = inputs.to(device, memory_format=torch.channels_last, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'), \\\n",
    "                     torch.autocast(device_type=device.type, dtype=AMP_DTYPE, enabled=True): \n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{torch.sum(preds == labels.data)/inputs.size(0):.4f}'})\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(f\"  {phase.upper()} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    try:\n",
    "                        torch.save(model.state_dict(), f'best_model_{phase_name.replace(\" \", \"_\").lower()}_temp.pth')\n",
    "                    except: pass\n",
    "        \n",
    "        if scheduler: scheduler.step()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"\\n{phase_name} Selesai: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s | Best Acc: {best_acc:.4f}\")\n",
    "    try: model.load_state_dict(best_model_wts)\n",
    "    except: pass\n",
    "    return model, history\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 2. Config & Data Loading\n",
    "DATASET_PATH = 'Dataset'\n",
    "IMG_SIZE = 224\n",
    "\n",
    "BATCH_SIZE_EXTRACT = 192\n",
    "BATCH_SIZE_TUNE = 12\n",
    "    \n",
    "EPOCHS_FEATURE_EXTRACT = 20\n",
    "EPOCHS_FINE_TUNE = 30\n",
    "    \n",
    "VALIDATION_SPLIT = 0.2 \n",
    "LR_FEATURE_EXTRACT = 1e-2 \n",
    "LR_FINE_TUNE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTHING = 0.1\n",
    "    \n",
    "PLOT_FILENAME = 'training_results_final.png'\n",
    "CONFUSION_MATRIX_FILENAME = 'confusion_matrix_final.png'\n",
    "BEST_MODEL_EXTRACT_PATH = 'best_model_extract.pth'\n",
    "MODEL_SAVE_PATH = 'citrus_efficientnetv2s_final.pth'\n",
    "\n",
    "class InMemoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        temp_dataset = datasets.ImageFolder(root_dir)\n",
    "        self.classes = temp_dataset.classes\n",
    "        self.targets = temp_dataset.targets\n",
    "        \n",
    "        print(f\"(!) Memuat {len(temp_dataset)} gambar ke RAM...\")\n",
    "        for path, target in tqdm(temp_dataset.samples, desc=\"Loading to RAM\"):\n",
    "            with open(path, 'rb') as f:\n",
    "                img = Image.open(f).convert('RGB')\n",
    "                img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "                self.samples.append((img, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.samples[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "def prepare_data():\n",
    "    weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "    preprocess = weights.transforms(antialias=True)\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        preprocess\n",
    "    ])\n",
    "    val_transforms = preprocess\n",
    "\n",
    "    try:\n",
    "        full_dataset = InMemoryDataset(DATASET_PATH, transform=None)\n",
    "        \n",
    "        CLASSES = sorted(full_dataset.classes)\n",
    "        NUM_CLASSES = len(CLASSES)\n",
    "        \n",
    "        print(\"=== DATASET INFO ===\")\n",
    "        print(f\"Total Gambar : {len(full_dataset)}\")\n",
    "        print(f\"Model        : EfficientNetV2-S\")\n",
    "        print(f\"Batch Size   : FE={BATCH_SIZE_EXTRACT}, FT={BATCH_SIZE_TUNE}\")\n",
    "        print(\"====================\")\n",
    "\n",
    "        targets = [s[1] for s in full_dataset.samples]\n",
    "        class_counts = np.bincount(targets)\n",
    "        class_weights = [len(full_dataset) / c for c in class_counts]\n",
    "        \n",
    "        train_size = int((1 - VALIDATION_SPLIT) * len(full_dataset))\n",
    "        val_size = len(full_dataset) - train_size\n",
    "        train_indices, val_indices = random_split(range(len(full_dataset)), [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        train_targets = [targets[i] for i in train_indices]\n",
    "        sample_weights = [class_weights[t] for t in train_targets]\n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(train_indices), replacement=True)\n",
    "\n",
    "        train_dataset = TransformedSubset(Subset(full_dataset, train_indices), train_transforms)\n",
    "        val_dataset = TransformedSubset(Subset(full_dataset, val_indices), val_transforms)\n",
    "\n",
    "        num_workers = 0 \n",
    "        \n",
    "        dataloaders_extract = {\n",
    "            'train': DataLoader(train_dataset, batch_size=BATCH_SIZE_EXTRACT, sampler=sampler, num_workers=num_workers, pin_memory=True),\n",
    "            'val': DataLoader(val_dataset, batch_size=BATCH_SIZE_EXTRACT, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        }\n",
    "        dataloaders_tune = {\n",
    "            'train': DataLoader(train_dataset, batch_size=BATCH_SIZE_TUNE, sampler=sampler, num_workers=num_workers, pin_memory=True),\n",
    "            'val': DataLoader(val_dataset, batch_size=BATCH_SIZE_TUNE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        }\n",
    "        dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "        \n",
    "        return dataloaders_extract, dataloaders_tune, dataset_sizes, CLASSES, NUM_CLASSES, weights\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error Dataset: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataloaders_extract, dataloaders_tune, dataset_sizes, class_names, num_classes, weights = prepare_data()\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9acedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 3. Feature Extraction\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n=== TAHAP 1: FEATURE EXTRACTION ===\")\n",
    "    \n",
    "    if 'weights' not in locals():\n",
    "         dataloaders_extract, dataloaders_tune, dataset_sizes, class_names, num_classes, weights = prepare_data()\n",
    "\n",
    "    model_extract = models.efficientnet_v2_s(weights=weights)\n",
    "\n",
    "    for i in range(len(model_extract.features)):\n",
    "        model_extract.features[i] = CheckpointWrapper(model_extract.features[i])\n",
    "    for param in model_extract.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_extract.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4, inplace=True),\n",
    "        nn.Linear(model_extract.classifier[1].in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    model_extract = model_extract.to(DEVICE, memory_format=torch.channels_last)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "    \n",
    "    optimizer = optim.SGD(model_extract.classifier.parameters(), lr=LR_FEATURE_EXTRACT, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FEATURE_EXTRACT)\n",
    "\n",
    "    model_extract, history_extract = train_model(\n",
    "        model_extract, criterion, optimizer, scheduler, dataloaders_extract, DEVICE, \n",
    "        EPOCHS_FEATURE_EXTRACT, dataset_sizes, phase_name=\"Feature Extraction\"\n",
    "    )\n",
    "    \n",
    "    torch.save(model_extract.state_dict(), BEST_MODEL_EXTRACT_PATH)\n",
    "    print(f\"Checkpoint tersimpan: {BEST_MODEL_EXTRACT_PATH}\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 4. Fine Tuning\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n=== TAHAP 2: FINE-TUNING ===\")\n",
    "    \n",
    "    if 'weights' not in locals():\n",
    "         dataloaders_extract, dataloaders_tune, dataset_sizes, class_names, num_classes, weights = prepare_data()\n",
    "         \n",
    "    model_tune = models.efficientnet_v2_s(weights=None) \n",
    "\n",
    "    num_ftrs_tune = model_tune.classifier[1].in_features\n",
    "    model_tune.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4, inplace=True),\n",
    "        nn.Linear(num_ftrs_tune, num_classes)\n",
    "    )\n",
    "\n",
    "    for i in range(len(model_tune.features)):\n",
    "        model_tune.features[i] = CheckpointWrapper(model_tune.features[i])\n",
    "\n",
    "    try:\n",
    "        model_tune.load_state_dict(torch.load(BEST_MODEL_EXTRACT_PATH, map_location='cpu'))\n",
    "        print(\"Bobot FE dimuat.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Load error: {e}, pakai init awal.\")\n",
    "        model_tune.load_state_dict(model_extract.state_dict())\n",
    "\n",
    "    for param in model_tune.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model_tune = model_tune.to(DEVICE, memory_format=torch.channels_last)\n",
    "\n",
    "    optimizer_tune = optim.SGD(model_tune.parameters(), lr=LR_FINE_TUNE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    scheduler_tune = optim.lr_scheduler.CosineAnnealingLR(optimizer_tune, T_max=EPOCHS_FINE_TUNE)\n",
    "\n",
    "    model_final, history_tune = train_model(\n",
    "        model_tune, criterion, optimizer_tune, scheduler_tune, dataloaders_tune, DEVICE, \n",
    "        EPOCHS_FINE_TUNE, dataset_sizes, phase_name=\"Fine-Tuning\"\n",
    "    )\n",
    "\n",
    "    torch.save(model_final.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Model Final tersimpan: {MODEL_SAVE_PATH}\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 5. Visualization & Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f\"\\n=== VISUALISASI & EVALUASI ===\")\n",
    "    \n",
    "    combined_history = {\n",
    "        'train_acc': history_extract.get('train_acc', []) + history_tune.get('train_acc', []),\n",
    "        'val_acc': history_extract.get('val_acc', []) + history_tune.get('val_acc', []),\n",
    "        'train_loss': history_extract.get('train_loss', []) + history_tune.get('train_loss', []),\n",
    "        'val_loss': history_extract.get('val_loss', []) + history_tune.get('val_loss', [])\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(combined_history['train_acc'], label='Train Acc')\n",
    "    plt.plot(combined_history['val_acc'], label='Val Acc', marker='o')\n",
    "    plt.title('Accuracy History')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(combined_history['train_loss'], label='Train Loss')\n",
    "    plt.plot(combined_history['val_loss'], label='Val Loss', marker='o')\n",
    "    plt.title('Loss History')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(PLOT_FILENAME)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nMenghitung Metrik Lengkap...\")\n",
    "    if 'model_final' in locals():\n",
    "        model_final.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(dataloaders_tune['val'], desc=\"Evaluasi Final\"):\n",
    "                inputs = inputs.to(DEVICE, memory_format=torch.channels_last)\n",
    "                labels = labels.to(DEVICE)\n",
    "                \n",
    "                with torch.autocast(device_type=DEVICE.type, dtype=torch.bfloat16, enabled=True):\n",
    "                    outputs = model_final(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LAPORAN KLASIFIKASI DETAIL\")\n",
    "        print(\"=\"*60)\n",
    "        print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
    "        \n",
    "        cf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        row_sums = cf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        row_sums[row_sums == 0] = 1 \n",
    "        percentages = cf_matrix.astype('float') / row_sums\n",
    "        \n",
    "        annot_labels = np.asarray([\n",
    "            [f\"{count}\\n({pct:.1%})\" for count, pct in zip(row, pct_row)]\n",
    "            for row, pct_row in zip(cf_matrix, percentages)\n",
    "        ])\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cf_matrix, annot=annot_labels, fmt='', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    annot_kws={\"size\": 10})\n",
    "        \n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix (Count & Percentage)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(CONFUSION_MATRIX_FILENAME)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Model final tidak ditemukan.\")\n",
    "\n",
    "    print(\"\\n--- PROSES SELESAI ---\")\n",
    "# endregion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
